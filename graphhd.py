# -*- coding: utf-8 -*-
"""GraphHD MUTAG .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNa2n2XhMqi7YGETSVCD9iM5nFeQLwzW
"""

!pip install torch_geometric

import numpy as np
import torch
from torch_geometric.datasets import TUDataset
from torch_geometric.utils import to_networkx
import networkx as nx
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 1. CONFIGURACIÓN DE HDC
D = 10000           # Dimensión de los hipervectores
rng = np.random.RandomState(42)

# 2. CARGAR MUTAG Y DETERMINAR MÁXIMO Nº NODOS
dataset = TUDataset(root='data/TUDataset', name='MUTAG')
max_nodes = max(data.num_nodes for data in dataset)

# 3. GENERAR HIPERVECTORES BASE PARA RANKING (uno por posición 0..max_nodes-1)
#    Hipervectores bipolares aleatorios en {-1, +1}^D
base_hvs = rng.choice([-1, +1], size=(max_nodes, D))

# 4. FUNCIÓN DE CODIFICACIÓN DE GRAPHHD
def encode_graph_hd(G: nx.Graph) -> np.ndarray:
    """
    Codifica un grafo G (NetworkX) a un hipervector de dimensión D usando GraphHD:
      1) Calcular PageRank para cada nodo.
      2) Ordenar nodos por PageRank (descendente).
      3) Asignar a cada nodo un hipervector base según su posición en el ranking.
      4) Para cada arista (u,v): binding = H_u * H_v.
      5) Sumar todos los binding y binarizar signo para hipervector final.
    """
    # 4.1. Calcular PageRank
    pr = nx.pagerank(G)
    # 4.2. Ordenar nodos por PageRank descendente
    sorted_nodes = sorted(pr.items(), key=lambda x: x[1], reverse=True)
    # 4.3. Asignar hipervector de ranking a cada nodo
    node_rank_hv = {}
    for rank_idx, (node, _) in enumerate(sorted_nodes):
        node_rank_hv[node] = base_hvs[rank_idx]
    # 4.4. Bundling de los binding de cada arista
    H_graph = np.zeros(D, dtype=np.int32)
    for u, v in G.edges():
        h_u = node_rank_hv[u]
        h_v = node_rank_hv[v]
        h_edge = h_u * h_v             # binding elemento-a-elemento
        H_graph += h_edge              # bundling (suma entera)
    # 4.5. Binarizar (signo) para obtener vect bipolar ±1
    H_graph = np.where(H_graph >= 0, 1, -1)
    return H_graph

# 5. CONVERTIR CADA EJEMPLO DE PyG A NetworkX + ETIQUETA
graphs = []
labels = []
for data in dataset:
    G_nx = to_networkx(data, to_undirected=True)
    # Asegurar que los nodos estén etiquetados 0..n-1
    G_nx = nx.convert_node_labels_to_integers(G_nx, ordering="sorted")
    graphs.append(G_nx)
    labels.append(int(data.y.item()))

labels = np.array(labels)

# 6. CODIFICAR TODOS LOS GRAFOS
encoded_hvs = np.array([encode_graph_hd(G) for G in graphs])

# 7. DIVIDIR EN TRAIN/TEST (estratificado por etiqueta)
X_train, X_test, y_train, y_test = train_test_split(
    encoded_hvs, labels,
    test_size=0.2,
    random_state=42,
    stratify=labels
)

# 8. ENTRENAR “HIPERVECTORES DE CLASE” (bundling de todos los vectores de esa clase)
class_hvs = {}
for c in np.unique(y_train):
    sum_hv = np.sum(X_train[y_train == c], axis=0)
    class_hvs[c] = np.where(sum_hv >= 0, 1, -1)

# 9. CLASIFICACIÓN EN TEST (similitud coseno vs cada hipervector de clase)
def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

y_pred = []
for hv in X_test:
    sims = {c: cosine_similarity(hv, class_hvs[c]) for c in class_hvs}
    pred_class = max(sims, key=sims.get)
    y_pred.append(pred_class)
y_pred = np.array(y_pred)

# 10. EVALUAR PRECISIÓN
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy HDC en MUTAG: {acc * 100:.2f}%")

!pip install torch-scatter torch-sparse torch-geometric --quiet

# —————— 2. Importar librerías necesarias ——————
import numpy as np
import torch
from torch_geometric.datasets import TUDataset
from torch_geometric.utils import to_networkx
import networkx as nx
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# —————— 3. Parámetros de HDC ——————
D = 10000         # Dimensión de los hipervectores
rng = np.random.RandomState(42)

# —————— 4. Cargar ENZYMES (PyG) ——————
dataset = TUDataset(root='data/TUDataset', name='ENZYMES')

# Determinar el número máximo de nodos en todos los grafos
max_nodes = max(data.num_nodes for data in dataset)

# —————— 5. Generar hipervectores base para ranking (tamaño = max_nodes) ——————
# Hipervectores bipolares aleatorios en {-1,+1}^D
base_hvs = rng.choice([-1, +1], size=(max_nodes, D))

# —————— 6. Función para codificar un grafo NetworkX → hipervector HDC ——————
def encode_graph_hd(G: nx.Graph) -> np.ndarray:
    """
    Codifica un grafo G (NetworkX) a un hipervector de dimensión D usando GraphHD:
      1) Calcular PageRank.
      2) Ordenar nodos por PageRank (descendente).
      3) Asignar a cada nodo un hipervector base según su posición en el ranking.
      4) Por cada arista (u,v): binding = H_u * H_v (producto elemento a elemento).
      5) Sumar todos los binding y binarizar signo → hipervector ±1 final.
    """
    # 6.1 Calcular PageRank
    pr = nx.pagerank(G)
    # 6.2 Ordenar nodos de mayor a menor PageRank
    sorted_nodes = sorted(pr.items(), key=lambda x: x[1], reverse=True)
    # 6.3 Asignar a cada nodo su hipervector base según su posición
    node_rank_hv = {}
    for rank_idx, (node, _) in enumerate(sorted_nodes):
        node_rank_hv[node] = base_hvs[rank_idx]
    # 6.4 Para cada arista, hacer binding y sumar (bundling)
    H_graph = np.zeros(D, dtype=np.int32)
    for u, v in G.edges():
        h_u = node_rank_hv[u]
        h_v = node_rank_hv[v]
        h_edge = h_u * h_v       # binding componente a componente
        H_graph += h_edge        # bundling (suma)
    # 6.5 Binarizar por signo para obtener vector bipolar ±1
    H_graph = np.where(H_graph >= 0, 1, -1)
    return H_graph

# —————— 7. Convertir cada ejemplo de PyG a NetworkX y extraer etiqueta ——————
graphs = []
labels = []
for data in dataset:
    # Convertir a grafo no dirigido de NetworkX
    G_nx = to_networkx(data, to_undirected=True)
    # Renumerar nodos a 0..n-1
    G_nx = nx.convert_node_labels_to_integers(G_nx, ordering="sorted")
    graphs.append(G_nx)
    labels.append(int(data.y.item()))

labels = np.array(labels)

# —————— 8. Codificar todos los grafos con HDC ——————
encoded_hvs = np.stack([encode_graph_hd(G) for G in graphs])

# —————— 9. Dividir en entrenamiento y prueba ——————
X_train, X_test, y_train, y_test = train_test_split(
    encoded_hvs, labels,
    test_size=0.2,
    random_state=42,
    stratify=labels
)

# —————— 10. Entrenar “hipervectores de clase” (multiclase) ——————
class_hvs = {}
for c in np.unique(y_train):
    sum_hv = np.sum(X_train[y_train == c], axis=0)   # bundling de todos los vectores de esa clase
    class_hvs[c] = np.where(sum_hv >= 0, 1, -1)       # binarizar

# —————— 11. Inferencia en test (similitud coseno) ——————
def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

y_pred = []
for hv in X_test:
    sims = {c: cosine_similarity(hv, class_hvs[c]) for c in class_hvs}
    pred_class = max(sims, key=sims.get)
    y_pred.append(pred_class)
y_pred = np.array(y_pred)

# —————— 12. Evaluar accuracy ——————
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy HDC en ENZYMES: {acc * 100:.2f}%")